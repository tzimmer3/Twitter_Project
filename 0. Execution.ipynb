{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "# Twitter Authentication\n",
    "from src import ACCESS_TOKEN, ACCESS_TOKEN_SECRET, CONSUMER_KEY, CONSUMER_SECRET\n",
    "\n",
    "# Twitter Scraping Class\n",
    "from src import TwitterClient\n",
    "\n",
    "# NLP and Metrics\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t_zim\\Desktop\\Data\\Twitter\\2023-01-31_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "# Create name of file\n",
    "\n",
    "dirname = \"C:\\\\Users\\\\t_zim\\\\Desktop\\\\Data\\\\Twitter\\\\\"\n",
    "\n",
    "# Just creates the string with {path} and {filename} ready to go\n",
    "OUT_FILE = os.path.join(dirname, \"{timestamp}_tweets.csv\".format(timestamp=date.today()))\n",
    "print(OUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetAnalyzer():\n",
    "    \"\"\"\n",
    "    Functionality for analyzing and categorizing content from tweets.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    def analyze_sentiment(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def tweets_to_data_frame(self, tweets):\n",
    "        df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['tweets'])\n",
    "\n",
    "        df['id'] = np.array([tweet.id for tweet in tweets])\n",
    "        df['tweet'] = np.array([tweet.text for tweet in tweets])\n",
    "        df['date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "        df['source'] = np.array([tweet.source for tweet in tweets])\n",
    "        df['likes'] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "        df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [User Input] - Input query terms\n",
    "- [Tweet Scraping] - Execute query on one term at a time (username or string) <- NOT DOING STRING YET\n",
    "- [Tweet Cleaning] - Clean tweets and format into dataframe\n",
    "- [Tweet Engineering] - Combine dataframes for all query terms in a subject area\n",
    "- [NLP and Metrics] - Compute metrics and transformations\n",
    "- [Visualize]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_screenname = \"realDonaldTrump\"\n",
    "#number_of_tweets = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_client = TwitterClient()\n",
    "#tweet_analyzer = TweetAnalyzer()\n",
    "\n",
    "#api = twitter_client.get_twitter_client_api()\n",
    "#api = twitter_client.get_user_timeline_tweets()\n",
    "\n",
    "#tweets = api.user_timeline(screen_name=twitter_screenname, count=number_of_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input a screen name and see their recent tweets\n",
    "#USER = 'gregyoung101'\n",
    "#Number_of_Tweets = 1\n",
    "\n",
    "#twitter_client = TwitterClient(USER)\n",
    "#print(twitter_client.get_user_timeline_tweets(Number_of_Tweets))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Remove extra columns\n",
    "- Remove unnecessary characters\n",
    "- Remove HTML tags, hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns needed:\n",
    "    # id, tweet, Screen Name, date, source, likes, retweets\n",
    "\n",
    "#df = tweet_analyzer.tweets_to_data_frame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['ourselves', 'here', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', \n",
    "                'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', \n",
    "                'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', \n",
    "                'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', \n",
    "                'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', \n",
    "                'them', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', \n",
    "                'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', \n",
    "                'myself', 'which', 'those', 'i', 'I','after', 'few', 'whom', 'being', 'if', 'theirs', 'my', 'against','a', 'by', 'doing', \n",
    "                'it', 'how', 'further', 'was', 'here','than','can', 'let', 'll', 'the', 'to', 'and', 'in', 'we', 'you ', 'of', 'on', 'this',\n",
    "                'be', 'our', 'or', 'it', 'we', 're']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS\n",
    "def clean_special_chars(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "    regex = \"#(\\w+)\"\n",
    "    hashtag_list = re.findall(regex, tweet)\n",
    "    return hashtag_list\n",
    "\n",
    "def extract_mentions(tweet):\n",
    "    regex = \"@(\\w+)\"\n",
    "    mentions_list = re.findall(regex, tweet)\n",
    "    return mentions_list\n",
    "\n",
    "def count_caption_words(tweet):\n",
    "    return sum([i.strip(string.punctuation).isalpha() for i in tweet.split()])\n",
    "\n",
    "def remove_stopwords(tweet, stopwords):\n",
    "    #tweet = [x.lower() for x in tweet]\n",
    "    tweet = [word for word in tweet if word not in stopwords]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\hlmq\\\\OneDrive - Chevron\\\\Desktop\\\\Projects\\\\Twitter News\\\\sample tweets\\\\tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pieces out of tweets and put into df columns\n",
    "df['hashtags'] = df['tweet'].apply(extract_hashtags)\n",
    "df['mentions'] = df['tweet'].apply(extract_mentions)\n",
    "\n",
    "# Clean special characters from tweet text\n",
    "df['tweet'] = df['tweet'].apply(clean_special_chars)\n",
    "\n",
    "# Remove stop words\n",
    "df['tweet_nostops'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "# Compute length of each tweet (exclude really short ones)\n",
    "df['tweet_length'] = df['tweet'].apply(count_caption_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure to keep?\n",
    "# Remove short tweets.  Require that tweet is minimum of 5 words long.\n",
    "df = df[df['tweet_length']>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tweet_nostops</th>\n",
       "      <th>tweet_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1620202199019130880</td>\n",
       "      <td>Do you MealPrep What d be on the menu if you w...</td>\n",
       "      <td>2023-01-30 23:28:00+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>2500</td>\n",
       "      <td>201</td>\n",
       "      <td>[MealPrep]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Do MealPrep What d menu going space We selecte...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1620190353604190210</td>\n",
       "      <td>On Tuesday Jan 31 Demo 2 astronauts Douglas Hu...</td>\n",
       "      <td>2023-01-30 22:40:56+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>6268</td>\n",
       "      <td>533</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>On Tuesday Jan 31 Demo 2 astronauts Douglas Hu...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1620172363726999555</td>\n",
       "      <td>Great question Our Milky Way galaxy will likel...</td>\n",
       "      <td>2023-01-30 21:29:27+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>[lwzc7799]</td>\n",
       "      <td>Great question Our Milky Way galaxy likely col...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1620158867958108161</td>\n",
       "      <td>You re looking at what may be hundreds or even...</td>\n",
       "      <td>2023-01-30 20:35:49+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>8422</td>\n",
       "      <td>1124</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ChandraXRay]</td>\n",
       "      <td>You looking may hundreds even thousands galaxi...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1620134198492401665</td>\n",
       "      <td>Great question A Sample Return Lander would la...</td>\n",
       "      <td>2023-01-30 18:57:48+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[TonyRoma1992, ExploreCosmos_, esa]</td>\n",
       "      <td>Great question A Sample Return Lander would la...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1620116242953822208</td>\n",
       "      <td>RT Someone understood the assignment It s offi...</td>\n",
       "      <td>2023-01-30 17:46:27+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>405</td>\n",
       "      <td>[MarsSampleReturn]</td>\n",
       "      <td>[NASAJPL, NASAPersevere]</td>\n",
       "      <td>RT Someone understood assignment It official d...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1620098547835367427</td>\n",
       "      <td>The poem set to fly in space aboard and how yo...</td>\n",
       "      <td>2023-01-30 16:36:08+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1002</td>\n",
       "      <td>116</td>\n",
       "      <td>[]</td>\n",
       "      <td>[librarycongress, adalimon, EuropaClipper, Eur...</td>\n",
       "      <td>The poem set fly space aboard parti</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1620096711875088384</td>\n",
       "      <td>A collaboration written in the stars We ve tea...</td>\n",
       "      <td>2023-01-30 16:28:50+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>2794</td>\n",
       "      <td>363</td>\n",
       "      <td>[]</td>\n",
       "      <td>[LibraryCongress, AdaLimon]</td>\n",
       "      <td>A collaboration written stars We ve teamed Poe...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1619779987204149249</td>\n",
       "      <td>We re sending a new crew to the Station NASA s...</td>\n",
       "      <td>2023-01-29 19:30:17+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>10048</td>\n",
       "      <td>1042</td>\n",
       "      <td>[Crew6]</td>\n",
       "      <td>[Space_Station, SpaceX]</td>\n",
       "      <td>We sending new crew Station NASA Crew6 mission...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1619374684826664960</td>\n",
       "      <td>Good idea to take care of it before it gets ev...</td>\n",
       "      <td>2023-01-28 16:39:46+00:00</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[eternallife2112]</td>\n",
       "      <td>Good idea take care gets even nebulous</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              tweet  \\\n",
       "0   1620202199019130880  Do you MealPrep What d be on the menu if you w...   \n",
       "1   1620190353604190210  On Tuesday Jan 31 Demo 2 astronauts Douglas Hu...   \n",
       "2   1620172363726999555  Great question Our Milky Way galaxy will likel...   \n",
       "4   1620158867958108161  You re looking at what may be hundreds or even...   \n",
       "5   1620134198492401665  Great question A Sample Return Lander would la...   \n",
       "6   1620116242953822208  RT Someone understood the assignment It s offi...   \n",
       "7   1620098547835367427  The poem set to fly in space aboard and how yo...   \n",
       "8   1620096711875088384  A collaboration written in the stars We ve tea...   \n",
       "9   1619779987204149249  We re sending a new crew to the Station NASA s...   \n",
       "10  1619374684826664960  Good idea to take care of it before it gets ev...   \n",
       "\n",
       "                         date              source  likes  retweets  \\\n",
       "0   2023-01-30 23:28:00+00:00            Sprinklr   2500       201   \n",
       "1   2023-01-30 22:40:56+00:00            Sprinklr   6268       533   \n",
       "2   2023-01-30 21:29:27+00:00     Twitter Web App     51         7   \n",
       "4   2023-01-30 20:35:49+00:00            Sprinklr   8422      1124   \n",
       "5   2023-01-30 18:57:48+00:00     Twitter Web App      6         2   \n",
       "6   2023-01-30 17:46:27+00:00     Twitter Web App      0       405   \n",
       "7   2023-01-30 16:36:08+00:00     Twitter Web App   1002       116   \n",
       "8   2023-01-30 16:28:50+00:00            Sprinklr   2794       363   \n",
       "9   2023-01-29 19:30:17+00:00            Sprinklr  10048      1042   \n",
       "10  2023-01-28 16:39:46+00:00  Twitter for iPhone     33         3   \n",
       "\n",
       "              hashtags                                           mentions  \\\n",
       "0           [MealPrep]                                                 []   \n",
       "1                   []                                                 []   \n",
       "2                   []                                         [lwzc7799]   \n",
       "4                   []                                      [ChandraXRay]   \n",
       "5                   []                [TonyRoma1992, ExploreCosmos_, esa]   \n",
       "6   [MarsSampleReturn]                           [NASAJPL, NASAPersevere]   \n",
       "7                   []  [librarycongress, adalimon, EuropaClipper, Eur...   \n",
       "8                   []                        [LibraryCongress, AdaLimon]   \n",
       "9              [Crew6]                            [Space_Station, SpaceX]   \n",
       "10                  []                                  [eternallife2112]   \n",
       "\n",
       "                                        tweet_nostops  tweet_length  \n",
       "0   Do MealPrep What d menu going space We selecte...            23  \n",
       "1   On Tuesday Jan 31 Demo 2 astronauts Douglas Hu...            16  \n",
       "2   Great question Our Milky Way galaxy likely col...            16  \n",
       "4   You looking may hundreds even thousands galaxi...            17  \n",
       "5   Great question A Sample Return Lander would la...            15  \n",
       "6   RT Someone understood assignment It official d...            17  \n",
       "7                 The poem set fly space aboard parti            13  \n",
       "8   A collaboration written stars We ve teamed Poe...            17  \n",
       "9   We sending new crew Station NASA Crew6 mission...            17  \n",
       "10             Good idea take care gets even nebulous            13  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for a subject area"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "- Topic Modeling (BERTopic)\n",
    "\n",
    "- Analyze Sentiment for each tweet\n",
    "- Count # of tweets in subject [GLOBAL]\n",
    "- Count # of tweets in topic [Local]\n",
    "- CountWordFrequency across subject\n",
    "- Log accounts (most frequent mentions or other?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "2023-01-31 15:58:46,268 - BERTopic - Reduced dimensionality\n",
      "2023-01-31 15:58:46,278 - BERTopic - Clustered reduced embeddings\n",
      "2023-01-31 15:58:46,320 - BERTopic - Reduced number of topics from 1 to 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Prepare embeddings\n",
    "docs = df['tweet'].tolist()\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\", min_df=1)\n",
    "\n",
    "topic_model = BERTopic(nr_topics = 3,\n",
    "                       language = 'english',\n",
    "                       vectorizer_model = vectorizer_model,\n",
    "                       calculate_probabilities = True,\n",
    "                       verbose=True)\n",
    "\n",
    "topics, probabilities = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count\n",
       "0     -1     19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_freq().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "#df['sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['tweets']])\n",
    "\n",
    "# Count total len of df\n",
    "total_tweets_in_subject = len(df)\n",
    "print(total_tweets_in_subject)\n",
    "\n",
    "# Count len of df where topic == some value\n",
    "total_tweets_in_topic = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountWordFrequency\n",
    "\n",
    "#Create DTM\n",
    "cv = CountVectorizer(ngram_range = (1,3))\n",
    "dtm = cv.fit_transform(df['tweet_nostops'])\n",
    "words = np.array(cv.get_feature_names())\n",
    "\n",
    "#Look at top 15 most frequent words\n",
    "freqs=dtm.sum(axis=0).A.flatten() \n",
    "index=np.argsort(freqs)[-10:]\n",
    "\n",
    "# Construct dataframe\n",
    "WordFreq = pd.DataFrame.from_records(list(zip(words[index], freqs[index]))) \n",
    "WordFreq.columns = ['Word', 'Freq']\n",
    "\n",
    "# Plot horizontal bar graph\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "WordFreq.sort_values(by='Freq').plot.barh(\n",
    "                      x='Word',\n",
    "                      y='Freq',\n",
    "                      ax=ax,\n",
    "                      color=\"deepskyblue\")\n",
    "\n",
    "plt.title(\"Most Common Words from this Subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "dtm = vectorizer.fit_transform(df['mentions']).toarray()\n",
    "mention = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "#Look at top 5 most frequent mentions\n",
    "freqs=dtm.sum(axis=0).flatten()\n",
    "index=np.argsort(freqs)[-5:] \n",
    "\n",
    "# Construct dataframe\n",
    "Mentions = pd.DataFrame.from_records(list(zip(mention[index], freqs[index]))) \n",
    "Mentions.columns = ['Mention', 'Frequency']\n",
    "\n",
    "Mentions.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "Mentions.reset_index(inplace=True, drop=True)\n",
    "Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee9cf5fd0714cb920d7340508c15063f95cfdc9bae6b029d25aa5c3349178639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
