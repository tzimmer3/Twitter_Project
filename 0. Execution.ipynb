{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\nlp_testing\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analytics/Metrics\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "## TWEEPY STUFF\n",
    "# Twitter Authentication\n",
    "from src import ACCESS_TOKEN, ACCESS_TOKEN_SECRET, CONSUMER_KEY, CONSUMER_SECRET\n",
    "# Twitter Scraping Class\n",
    "from src import TwitterClient\n",
    "# Class for cleanup\n",
    "from src import TweetAnalyzer\n",
    "# NLP and Metrics\n",
    "\n",
    "# Visualization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [User Input] - Input query terms\n",
    "- [Tweet Scraping] - Execute query on one term at a time (username or string) <- NOT DOING STRING YET\n",
    "- [Tweet Cleaning] - Clean tweets and format into dataframe\n",
    "- [Tweet Engineering] - Combine dataframes for all query terms in a subject area\n",
    "- [NLP and Metrics] - Compute metrics and transformations\n",
    "- [Visualize]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT = \"Space\"\n",
    "\n",
    "Number_of_Tweets = 2\n",
    "\n",
    "# Input a screen name and see their recent tweets\n",
    "USER = 'NASA'\n",
    "\n",
    "# Input a list of query terms and GET tweets\n",
    "QUERY = ['NASA', 'space station', 'hubble']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By User Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config params\n",
    "twitter_client = TwitterClient(twitter_user=USER)\n",
    "\n",
    "# Print if you wish\n",
    "#print(twitter_client.get_user_timeline_tweets(Number_of_Tweets))\n",
    "\n",
    "# Retrieve tweets\n",
    "tweets = twitter_client.get_user_timeline_tweets(Number_of_Tweets)\n",
    "# Format tweets into DF\n",
    "df = TweetAnalyzer.tweets_to_data_frame(tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Search Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config params\n",
    "twitter_client = TwitterClient(query_term=QUERY)\n",
    "\n",
    "# Print if you wish\n",
    "#print(twitter_client.get_query_tweets(Number_of_Tweets))\n",
    "\n",
    "# Retrieve tweets\n",
    "tweets = twitter_client.get_query_tweets(Number_of_Tweets)\n",
    "# Format tweets into DF\n",
    "df = TweetAnalyzer.tweets_to_data_frame(tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Remove extra columns\n",
    "- Remove unnecessary characters\n",
    "- Remove HTML tags, hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## FUNCTIONS\n",
    "def clean_special_chars(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "    regex = \"#(\\w+)\"\n",
    "    hashtag_list = re.findall(regex, tweet)\n",
    "    return hashtag_list\n",
    "\n",
    "def extract_mentions(tweet):\n",
    "    regex = \"@(\\w+)\"\n",
    "    mentions_list = re.findall(regex, tweet)\n",
    "    return mentions_list\n",
    "\n",
    "def count_caption_words(tweet):\n",
    "    return sum([i.strip(string.punctuation).isalpha() for i in tweet.split()])\n",
    "\n",
    "def remove_stopwords(tweet, stopwords):\n",
    "    #tweet = [x.lower() for x in tweet]\n",
    "    tweet = [word for word in tweet if word not in stopwords]\n",
    "    return tweet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\hlmq\\\\OneDrive - Chevron\\\\Desktop\\\\Projects\\\\Twitter News\\\\sample tweets\\\\tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pieces out of tweets and put into df columns\n",
    "df['hashtags'] = df['tweet'].apply(TweetAnalyzer.extract_hashtags)\n",
    "df['mentions'] = df['tweet'].apply(TweetAnalyzer.extract_mentions)\n",
    "\n",
    "# Clean special characters from tweet text\n",
    "df['tweet'] = df['tweet'].apply(TweetAnalyzer.clean_special_chars)\n",
    "\n",
    "# Remove stop words\n",
    "## NOT NEEDED AT THIS TIME\n",
    "#df['tweet_nostops'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "# Compute length of each tweet (exclude really short ones)\n",
    "df['tweet_length'] = df['tweet'].apply(TweetAnalyzer.count_caption_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure to keep?\n",
    "# Remove short tweets.  Require that tweet is minimum of 5 words long.\n",
    "df = df[df['tweet_length']>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tweet_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1620202199019130880</td>\n",
       "      <td>Do you MealPrep What d be on the menu if you w...</td>\n",
       "      <td>2023-01-30 23:28:00+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>2500</td>\n",
       "      <td>201</td>\n",
       "      <td>[MealPrep]</td>\n",
       "      <td>[]</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1620190353604190210</td>\n",
       "      <td>On Tuesday Jan 31 Demo 2 astronauts Douglas Hu...</td>\n",
       "      <td>2023-01-30 22:40:56+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>6268</td>\n",
       "      <td>533</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1620172363726999555</td>\n",
       "      <td>Great question Our Milky Way galaxy will likel...</td>\n",
       "      <td>2023-01-30 21:29:27+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>[lwzc7799]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1620158867958108161</td>\n",
       "      <td>You re looking at what may be hundreds or even...</td>\n",
       "      <td>2023-01-30 20:35:49+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>8422</td>\n",
       "      <td>1124</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ChandraXRay]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1620134198492401665</td>\n",
       "      <td>Great question A Sample Return Lander would la...</td>\n",
       "      <td>2023-01-30 18:57:48+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[TonyRoma1992, ExploreCosmos_, esa]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1620116242953822208</td>\n",
       "      <td>RT Someone understood the assignment It s offi...</td>\n",
       "      <td>2023-01-30 17:46:27+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>405</td>\n",
       "      <td>[MarsSampleReturn]</td>\n",
       "      <td>[NASAJPL, NASAPersevere]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1620098547835367427</td>\n",
       "      <td>The poem set to fly in space aboard and how yo...</td>\n",
       "      <td>2023-01-30 16:36:08+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1002</td>\n",
       "      <td>116</td>\n",
       "      <td>[]</td>\n",
       "      <td>[librarycongress, adalimon, EuropaClipper, Eur...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1620096711875088384</td>\n",
       "      <td>A collaboration written in the stars We ve tea...</td>\n",
       "      <td>2023-01-30 16:28:50+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>2794</td>\n",
       "      <td>363</td>\n",
       "      <td>[]</td>\n",
       "      <td>[LibraryCongress, AdaLimon]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1619779987204149249</td>\n",
       "      <td>We re sending a new crew to the Station NASA s...</td>\n",
       "      <td>2023-01-29 19:30:17+00:00</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>10048</td>\n",
       "      <td>1042</td>\n",
       "      <td>[Crew6]</td>\n",
       "      <td>[Space_Station, SpaceX]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1619374684826664960</td>\n",
       "      <td>Good idea to take care of it before it gets ev...</td>\n",
       "      <td>2023-01-28 16:39:46+00:00</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[eternallife2112]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              tweet  \\\n",
       "0   1620202199019130880  Do you MealPrep What d be on the menu if you w...   \n",
       "1   1620190353604190210  On Tuesday Jan 31 Demo 2 astronauts Douglas Hu...   \n",
       "2   1620172363726999555  Great question Our Milky Way galaxy will likel...   \n",
       "4   1620158867958108161  You re looking at what may be hundreds or even...   \n",
       "5   1620134198492401665  Great question A Sample Return Lander would la...   \n",
       "6   1620116242953822208  RT Someone understood the assignment It s offi...   \n",
       "7   1620098547835367427  The poem set to fly in space aboard and how yo...   \n",
       "8   1620096711875088384  A collaboration written in the stars We ve tea...   \n",
       "9   1619779987204149249  We re sending a new crew to the Station NASA s...   \n",
       "10  1619374684826664960  Good idea to take care of it before it gets ev...   \n",
       "\n",
       "                         date              source  likes  retweets  \\\n",
       "0   2023-01-30 23:28:00+00:00            Sprinklr   2500       201   \n",
       "1   2023-01-30 22:40:56+00:00            Sprinklr   6268       533   \n",
       "2   2023-01-30 21:29:27+00:00     Twitter Web App     51         7   \n",
       "4   2023-01-30 20:35:49+00:00            Sprinklr   8422      1124   \n",
       "5   2023-01-30 18:57:48+00:00     Twitter Web App      6         2   \n",
       "6   2023-01-30 17:46:27+00:00     Twitter Web App      0       405   \n",
       "7   2023-01-30 16:36:08+00:00     Twitter Web App   1002       116   \n",
       "8   2023-01-30 16:28:50+00:00            Sprinklr   2794       363   \n",
       "9   2023-01-29 19:30:17+00:00            Sprinklr  10048      1042   \n",
       "10  2023-01-28 16:39:46+00:00  Twitter for iPhone     33         3   \n",
       "\n",
       "              hashtags                                           mentions  \\\n",
       "0           [MealPrep]                                                 []   \n",
       "1                   []                                                 []   \n",
       "2                   []                                         [lwzc7799]   \n",
       "4                   []                                      [ChandraXRay]   \n",
       "5                   []                [TonyRoma1992, ExploreCosmos_, esa]   \n",
       "6   [MarsSampleReturn]                           [NASAJPL, NASAPersevere]   \n",
       "7                   []  [librarycongress, adalimon, EuropaClipper, Eur...   \n",
       "8                   []                        [LibraryCongress, AdaLimon]   \n",
       "9              [Crew6]                            [Space_Station, SpaceX]   \n",
       "10                  []                                  [eternallife2112]   \n",
       "\n",
       "    tweet_length  \n",
       "0             23  \n",
       "1             16  \n",
       "2             16  \n",
       "4             17  \n",
       "5             15  \n",
       "6             17  \n",
       "7             13  \n",
       "8             17  \n",
       "9             17  \n",
       "10            13  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for a subject area"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "- Topic Modeling (BERTopic)\n",
    "\n",
    "- Analyze Sentiment for each tweet\n",
    "- Count # of tweets in subject [GLOBAL]\n",
    "- Count # of tweets in topic [Local]\n",
    "- CountWordFrequency across subject\n",
    "- Log accounts (most frequent mentions or other?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embeddings\n",
    "docs = df['tweet'].tolist()\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\", min_df=2)\n",
    "\n",
    "topic_model = BERTopic(nr_topics = 3, #max number of topics\n",
    "                       language = 'english',\n",
    "                       vectorizer_model = vectorizer_model,\n",
    "                       calculate_probabilities = True,\n",
    "                       verbose=True)\n",
    "\n",
    "topics, probabilities = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic -1 is the catch-all bin for outliers.\n",
    "# Need to modify min_df.  Believe this is the minimum number of obs to create a topic.\n",
    "# From word frequency, believe ngram = 2 is good.\n",
    "# ACTION: Generate more tweets and test with different min_df value.\n",
    "\n",
    "topic_model.get_topic_freq().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "#df['sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['tweets']])\n",
    "\n",
    "# Count total len of df\n",
    "total_tweets_in_subject = len(df)\n",
    "print(total_tweets_in_subject)\n",
    "\n",
    "# Count len of df where topic == some value\n",
    "total_tweets_in_topic = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountWordFrequency\n",
    "\n",
    "#Create DTM\n",
    "cv = CountVectorizer(ngram_range = (1,3), stop_words='english')\n",
    "dtm = cv.fit_transform(df['tweet'])\n",
    "words = np.array(cv.get_feature_names())\n",
    "\n",
    "#Look at top 15 most frequent words\n",
    "freqs=dtm.sum(axis=0).A.flatten() \n",
    "index=np.argsort(freqs)[-10:]\n",
    "\n",
    "# Construct dataframe\n",
    "WordFreq = pd.DataFrame.from_records(list(zip(words[index], freqs[index]))) \n",
    "WordFreq.columns = ['Word', 'Freq']\n",
    "\n",
    "# Plot horizontal bar graph\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "WordFreq.sort_values(by='Freq').plot.barh(\n",
    "                      x='Word',\n",
    "                      y='Freq',\n",
    "                      ax=ax,\n",
    "                      color=\"deepskyblue\")\n",
    "\n",
    "plt.title(\"Most Common Words from this Subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "dtm = vectorizer.fit_transform(df['mentions']).toarray()\n",
    "mention = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "#Look at top 5 most frequent mentions\n",
    "freqs=dtm.sum(axis=0).flatten()\n",
    "index=np.argsort(freqs)[-5:] \n",
    "\n",
    "# Construct dataframe\n",
    "Mentions = pd.DataFrame.from_records(list(zip(mention[index], freqs[index]))) \n",
    "Mentions.columns = ['Mention', 'Frequency']\n",
    "\n",
    "Mentions.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "Mentions.reset_index(inplace=True, drop=True)\n",
    "Mentions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create name of file\n",
    "\n",
    "dirname = \"C:\\\\Users\\\\t_zim\\\\Desktop\\\\Data\\\\Twitter\\\\\"\n",
    "\n",
    "# Just creates the string with {path} and {filename} ready to go\n",
    "OUT_FILE = os.path.join(dirname, \"{timestamp}_{subject}tweets.csv\".format(timestamp=date.today(), subject=SUBJECT))\n",
    "print(OUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUT_FILE, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee9cf5fd0714cb920d7340508c15063f95cfdc9bae6b029d25aa5c3349178639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
